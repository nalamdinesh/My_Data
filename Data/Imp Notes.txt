set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask.size=10000000
set hive.cbo.enable=true;
set hive.exec.parallel=true;
set hive.execution.engine=tez;

To run spark job:
=======================
to create jar from IntelliJ :
---------------------------
1. save the project
2. go to project structure ---> click on artifacts-----> click on green + sign ---> select jar ---> with all dependencies
3. select main class for which we need to create jar--->select path and then apply and then ok
4. from main menu click on Build--->Build artifacts---> build---> it will create the jar in specified path
5. move/copy that jar file to name node (/home/thotave5) and then submit your spark application


spark-submit --class com.novartis.DROID.programs.Promacta_NonPersonal_PROMACTA_HCP_ENL --master yarn \
--deploy-mode cluster --executor-memory 1G --num-executors 2 \
/home/nalamdi1/droidSparkJars/prm_hcp_enl_jar/DROID.jar

after submitting if you get any error like exit code 10(Container exited with a non-zero exit code 10)
then run the below command and again submit the job

zip -d /home/nalamdi1/droidSparkJars/prm_enl_jar/DROID.jar META-INF/*.RSA META-INF/*.DSA META-INF/*.SF

 it will delete unwanted files and then run the spark job again 
 
spark-submit --class com.novartis.DROID.programs.Promacta_NonPersonal_PROMACTA_HCP_ENL --master yarn \
--deploy-mode cluster --executor-memory 1G --num-executors 2 \
/home/nalamdi1/droidSparkJars/prm_hcp_enl_jar/DROID.jar

========================================================================================
Spark Errors

org.apache.spark.sql.AnalysisException: Detected cartesian product for INNER join between logical plans

spark.conf.set("spark.sql.crossJoin.enabled","true")

org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:
spark.conf.set("spark.sql.broadcastTimeout","1200")



